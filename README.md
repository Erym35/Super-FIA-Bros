# Super Mario Bros RL ‚Äì PPO Agent

## üìã Descrizione

Questo progetto implementa un agente di **Reinforcement Learning (RL)** capace di imparare a giocare a **Super Mario Bros (NES)** utilizzando l‚Äôalgoritmo **PPO (Proximal Policy Optimization)**.

Il progetto √® configurato per funzionare su **Windows 11**, superando le limitazioni di compatibilit√† delle librerie `nes-py` e `gym` tramite un ambiente **Python 3.11** specifico.


## ‚öôÔ∏è Prerequisiti

Realizzare un agente in grado di **completare il livello 1-1 di Super Mario Bros**, confrontando due pipeline algoritmiche differenti e analizzandone:

### Visual Studio Build Tools

Necessari per compilare i componenti C++ dell‚Äôemulatore.
Durante l‚Äôinstallazione selezionare il carico di lavoro:

* **Sviluppo desktop con C++**

### Python 3.11.x

### üîµ PPO ‚Äì Deep Reinforcement Learning

* Algoritmo: Proximal Policy Optimization (PPO)
* Policy: CNN (CnnPolicy)
* Input: frame preprocessati (grayscale, resize, frame stacking)
* Reward progettata per incentivare avanzamento e completamento del livello

Il codice e i risultati relativi a questa pipeline sono disponibili nel branch:

```
PPO_model
```

---

### üü£ NEAT ‚Äì Neuroevoluzione

* Algoritmo: NEAT (NeuroEvolution of Augmenting Topologies)
* Evoluzione di pesi e topologia della rete neurale
* Fitness basata sulla distanza percorsa (x_pos)
* Speciazione, elitismo e stagnazione configurati esplicitamente

Questa pipeline √® stata integrata e valutata sul livello 1-1.

Il codice e i risultati relativi a questa pipeline sono disponibili nel branch:

```
NEAT_model
```

---

### 2Ô∏è‚É£ Attivazione

Attiva l'ambiente virtuale:

```powershell
.\mario_311\Scripts\activate
```

---

### 3Ô∏è‚É£ Installazione Dipendenze

L‚Äôordine di installazione √® **critico** per evitare conflitti su Windows. Esegui i comandi in sequenza:

#### A. Setup compilatori e compatibilit√†

```powershell
pip install setuptools==65.5.0 wheel<0.40.0
```

#### B. Emulatore e ambiente di gioco

```powershell
pip install nes-py
pip install gym_super_mario_bros==7.4.0
```

#### C. Librerie di Reinforcement Learning

```powershell
pip install gymnasium stable-baselines3[extra] shimmy
```

* distanza percorsa sull‚Äôasse orizzontale (x_pos)
* completamento del livello (bandiera finale)
* andamento dell‚Äôapprendimento (TensorBoard per PPO)
* evoluzione della fitness media (`avg_fitness.svg` per NEAT)
* costi computazionali e tempo di training

L‚Äôanalisi completa √® riportata nella documentazione.

---

## üìÅ Struttura della repository (branch `main`)

```text
Super-FIA-Bros/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ docs/
‚îî‚îÄ‚îÄ notebooks/
```

Le implementazioni specifiche dei modelli sono separate nei branch dedicati.

---

## üß† Struttura del Training

Una volta aperto Jupyter Lab, creare un nuovo notebook selezionando il kernel **Python (Mario 3.11)**.
Il flusso di lavoro √® suddiviso in quattro celle logiche:

* **Import**: caricamento delle librerie (`gym`, `stable_baselines3`, `cv2`).
* **Preprocessing (Wrappers)**: conversione in scala di grigi (84√ó84).
* **Frame Stacking**: utilizzo di 4 frame consecutivi per percepire movimento e velocit√†.
* **Definizione Modello**: utilizzo di PPO (`CnnPolicy`) con iperparametri ottimizzati.

Setup cartelle per i log:

```python
tensorboard_log = "./logs/"
```

Training loop:

```python
model.learn(total_timesteps=1000000)
```

(con salvataggio periodico dei checkpoint)

---

## üìà TensorBoard

Per visualizzare i grafici di apprendimento (aumento del Reward, diminuzione della Loss, ecc.) in tempo reale, mentre L'IA si allena:

1. Aprire un nuovo terminale PowerShell (lasciando quello del training in esecuzione);
2. Attivare l‚Äôambiente virtuale;
3. Eseguire il comando puntando alla cartella dei log:

```powershell
tensorboard --logdir=./logs/
```

Aprire il browser all‚Äôindirizzo indicato (solitamente `http://localhost:6006`).

---

## üõë Risoluzione Problemi Comuni

**Errore: ‚ÄúMicrosoft Visual C++ 14.0 is required‚Äù**
Verificare di aver installato i Build Tools (punto 1 dei Prerequisiti).

Per esplorare le implementazioni:

1. Passare al branch di interesse:

   * `PPO_model`
   * `NEAT_model`
2. Seguire le istruzioni presenti nei file e nei notebook del branch selezionato.

---

## ‚ÑπÔ∏è Note Finali

* [Luca Afeltra](https://github.com/luca-afe)
* [Matteo De Stasio](https://github.com/Matteo-d-s)
* [Marianna Diograzia](https://github.com/Erym35)
